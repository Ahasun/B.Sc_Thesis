{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EOI0SeHaX22"
      },
      "source": [
        "!pip install pyspellchecker\n",
        "!pip install demoji\n",
        "!pip install emot\n",
        "!pip install ekphrasis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3OvJ_efuR1I"
      },
      "source": [
        "# Import Section\n",
        "import csv, codecs, sys, io, re, random, string\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import demoji\n",
        "from emot.emo_unicode import EMOTICONS_EMO\n",
        "from emot.core import emot\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer(preserve_case=True, strip_handles=True, reduce_len=True)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "# For Classifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from statistics import mean\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# Python script for confusion matrix creation. \n",
        "from sklearn.metrics import *\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU2yfhfxb8aE"
      },
      "source": [
        "data = pd.read_csv(\"/content/emotions.txt\",quoting=csv.QUOTE_NONE)\n",
        "\n",
        "train= pd.read_csv(\"/content/train.tsv\",sep='\\t',skipinitialspace=False)\n",
        "train=pd.DataFrame(train.values,columns=[\"Reddit\",\"Label\",\"Comment_Id\"])\n",
        "\n",
        "test= pd.read_csv(\"/content/test.tsv\",sep='\\t',skipinitialspace=False)\n",
        "test=pd.DataFrame(test.values,columns=[\"Reddit\",\"Label\",\"Comment_Id\"])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waiGIMq6zFkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d801cb2-4647-4879-b2f5-9b9be83b512d"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "#Global Initialization Section\n",
        "seg = Segmenter(corpus=\"twitter\")\n",
        "\n",
        "def hashtagSegmentation(text):\n",
        "    givenText=text\n",
        "    getHashTagFromText = [t for t in givenText.split() if t.startswith('#')]\n",
        "    segment_the_Hash =''\n",
        "    getHashtagSegmentedText = givenText\n",
        "    #getHashtagSegmentedText = \" \".join( [t for t in givenText.split() if not t.startswith('#')])\n",
        "\n",
        "    if getHashTagFromText:\n",
        "        for eachHashTag in getHashTagFromText:\n",
        "             eachHashTag=eachHashTag[1:]\n",
        "             segment_the_Hash = seg.segment(eachHashTag)\n",
        "             getHashtagSegmentedText = getHashtagSegmentedText+\" \"+ segment_the_Hash\n",
        "\n",
        "    return getHashtagSegmentedText\n",
        "\n",
        "def nomalize_reddit_text(reddit):\n",
        "    reddit = reddit.lower()\n",
        "    # Remove urls\n",
        "    reddit = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', reddit, flags=re.MULTILINE)\n",
        "    # Remove user @ references and '#' from tweet\n",
        "    reddit = re.sub(r'[0-9]+|\\#','',reddit)\n",
        "    # Remove punctuations\n",
        "    reddit = reddit.translate(str.maketrans('','', string.punctuation+'â€™'))\n",
        "    return reddit\n",
        "\n",
        "def preprocess_reddit_text(tweet):\n",
        "    #Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    #Tokenizer\n",
        "    tweet_tokens = tweet_tokenizer.tokenize(tweet)\n",
        "    #tweet_tokens = word_tokenize(tweet)\n",
        "\n",
        "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
        "    #ps = PorterStemmer()\n",
        "    #stemmed_words = [ps.stem(w) for w in ks_stemmed_words]\n",
        "    from nltk.stem.snowball import SnowballStemmer \n",
        "    #the stemmer requires a language parameter \n",
        "    ss= SnowballStemmer(language='english')\n",
        "    stemmed_words = [ss.stem(w) for w in filtered_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
        "    \n",
        "    return \" \".join(lemma_words)\n",
        "\n",
        "def convert_emoticons(text):\n",
        "    tweet=demoji.replace_with_desc(text,\" \")\n",
        "    for emot in EMOTICONS_EMO:\n",
        "        #text = re.sub(u'['+emot+']', \"\".join(EMOTICONS_EMO[emot].replace(\",\",\"\").split()), text1)\n",
        "        text= text.replace(emot, EMOTICONS_EMO[emot])\n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-psGx1EvxQ7G"
      },
      "source": [
        "# train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEDafzvZfD0G"
      },
      "source": [
        "train.Reddit = train['Reddit'].apply(convert_emoticons)\n",
        "train.Reddit = train['Reddit'].apply(hashtagSegmentation)\n",
        "train.Reddit = train['Reddit'].apply(preprocess_reddit_text)\n",
        "train.Reddit = train['Reddit'].apply(nomalize_reddit_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWg6KfbFfGKR"
      },
      "source": [
        "# train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sksDQLVnh6tE"
      },
      "source": [
        "test.Reddit = test['Reddit'].apply(convert_emoticons)\n",
        "test.Reddit = test['Reddit'].apply(hashtagSegmentation)\n",
        "test.Reddit = test['Reddit'].apply(preprocess_reddit_text)\n",
        "test.Reddit = test['Reddit'].apply(nomalize_reddit_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJfzf-4-HHHC"
      },
      "source": [
        "# for i in range(len(test)):\n",
        "#   print(test['Reddit'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W51ZF8cDwvlu"
      },
      "source": [
        "# test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc1BQ5Sjs9_I"
      },
      "source": [
        "d=data['Emotion_Name']\n",
        "M_data=pd.DataFrame(d,columns=[\"Emotion_Name\"])\n",
        "#M_data = pd.concat([ttemp_data, data]).reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IodMv8WQbfZf"
      },
      "source": [
        "M_data['Emotion_Name']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckTyBGqt7jLx"
      },
      "source": [
        "#tmp=train.drop[['Label','Comment_Id'], axis=1]\n",
        "\n",
        "# df_label = DataLoad.drop(['AnimeName', 'SearchContents'], axis=1)\n",
        "a=train['Reddit']\n",
        "array=[]\n",
        "\n",
        "for i in range(len(train)):\n",
        "  temp=[]\n",
        "  #temp.append(a[i])\n",
        "  for j in range(28):\n",
        "    temp.append(0)\n",
        "\n",
        "  array.append(temp)\n",
        "\n",
        "X=train['Label']\n",
        "\n",
        "for i in range(len(train)):\n",
        "  b=X[i].split(',')\n",
        "  for j in range(len(b)):\n",
        "    array[i][int(b[j])]=1\n",
        "\n",
        "array_df=pd.DataFrame(array,columns=M_data['Emotion_Name'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycQyfF3duZ8e"
      },
      "source": [
        "# array_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzy51pYElKyy"
      },
      "source": [
        "b=test['Reddit']\n",
        "brray=[]\n",
        "\n",
        "for i in range(len(test)):\n",
        "  temp=[]\n",
        "  #temp.append(b[i])\n",
        "  for j in range(28):\n",
        "    temp.append(0)\n",
        "\n",
        "  brray.append(temp)\n",
        "\n",
        "X=test['Label']\n",
        "\n",
        "for i in range(len(test)):\n",
        "  c=X[i].split(',')\n",
        "  for j in range(len(c)):\n",
        "    brray[i][int(c[j])]=1\n",
        "\n",
        "brray_df=pd.DataFrame(brray,columns=M_data['Emotion_Name'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbo66L16vvw2"
      },
      "source": [
        "# brray_df\n",
        "#print(len(test.Reddit))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spzR7dvdRa5J"
      },
      "source": [
        "tmp_train=array_df\n",
        "tmp_test=brray_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya1tex3Sb_lj"
      },
      "source": [
        "#data\n",
        "#N_data\n",
        "#categories = N_data['Emotion_Name'];\n",
        "# #categories.pop(categories.index[0])\n",
        "\n",
        "# print(N_data['Emotion_Name'])\n",
        "\n",
        "categories=M_data['Emotion_Name']\n",
        "#categories=categories.iloc[1:]\n",
        "# for a in categories:\n",
        "#   print(a)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R10UOgwG1lLg"
      },
      "source": [
        "# for i in len(test):\n",
        "#   print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNWH0EsGzdBs"
      },
      "source": [
        "X_train = train['Reddit']\n",
        "X_test = test['Reddit']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6syJuvRvVsG"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDSzJaYzrWXc"
      },
      "source": [
        "#**MultinomialNB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcRpoj2UgJaM"
      },
      "source": [
        "\n",
        "\n",
        "  ## Define Classifier\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "\n",
        "\n",
        "\n",
        "classifier = Pipeline([\n",
        "      ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "      ('clf', OneVsRestClassifier(MultinomialNB()))])\n",
        "\n",
        "\n",
        "\n",
        "# print(X_train)\n",
        "\n",
        "\n",
        "df_predicted=pd.DataFrame()\n",
        "df_groundTruth=pd.DataFrame()\n",
        "\n",
        "for category in categories:\n",
        "  #print('... Processing Category: {}'.format(category))\n",
        "  # train the model using X_dtm & y\n",
        "  classifier.fit(X_train, tmp_train[category])\n",
        "  \n",
        "# compute the testing accuracy\n",
        "  prediction = classifier.predict(X_test)\n",
        "  df_predicted[category]=prediction\n",
        "  df_groundTruth[category]=tmp_test[category]\n",
        "\n",
        "  # s=(df_predicted[category]==Test[category]).sum()\n",
        "  # print(s)\n",
        "  # print('F1 Micro: {}'.format(f1_score(test[category], prediction, average='micro')))\n",
        "\n",
        "\n",
        "# print(df_predicted)\n",
        "# print(df_groundTruth)\n",
        "\n",
        "y_true = np.array(df_groundTruth)\n",
        "y_pred = np.array(df_predicted)\n",
        "\n",
        "\n",
        "\n",
        "# print(y_true)\n",
        "# print(y_pred)\n",
        "\n",
        "# print(\"\\n\")\n",
        "print(\"F1_Micro:\", f1_score(y_true, y_pred, average='micro'))\n",
        "print(\"F1_Macro:\", f1_score(y_true, y_pred, average='macro'))\n",
        "print(\"Multi-label Accuracy (or Jaccard Index):\", jaccard_score(y_true,y_pred, average='samples'))\n",
        "print(\"Hamming_loss:\", hamming_loss(y_true, y_pred)) \n",
        "print(\"Precision_score\",precision_score(y_true, y_pred, average='samples'))   \n",
        "print(\"Recall_score\",recall_score(y_true, y_pred, average='samples'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkNtzk-HrMYC"
      },
      "source": [
        "#**LinearSVC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIjIj8foziac"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "  ## Define Classifier\n",
        "# Ref.: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
        "\n",
        "# from sklearn.svm import LinearSVC\n",
        "# from sklearn.svm import SVC\n",
        "\n",
        "classifier = Pipeline([\n",
        "     ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "     ('tfidf', TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)),\n",
        "     ('clf', OneVsRestClassifier(LinearSVC(C=10.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "      intercept_scaling=1, loss='squared_hinge', max_iter=1500,\n",
        "      multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
        "      verbose=0)))])\n",
        "\n",
        "\n",
        "# print(X_train)\n",
        "\n",
        "\n",
        "df_predicted=pd.DataFrame()\n",
        "df_groundTruth=pd.DataFrame()\n",
        "\n",
        "for category in categories:\n",
        "  print('... Processing Category: {}'.format(category))\n",
        "  # train the model using X_dtm & y\n",
        "  classifier.fit(X_train, tmp_train[category])\n",
        "  \n",
        "# compute the testing accuracy\n",
        "  prediction = classifier.predict(X_test)\n",
        "  # print(prediction)\n",
        "  df_predicted[category]=prediction\n",
        "  df_groundTruth[category]=tmp_test[category]\n",
        "  # print('F1 Micro: {}'.format(f1_score(test[category], prediction, average='micro')))\n",
        "\n",
        "\n",
        "# print(df_predicted)\n",
        "# print(df_groundTruth)\n",
        "\n",
        "y_true = np.array(df_groundTruth)\n",
        "y_pred = np.array(df_predicted)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(y_true)\n",
        "# print(y_pred)\n",
        "\n",
        "# print(\"\\n\")\n",
        "print(\"F1_Micro:\", f1_score(y_true, y_pred, average='micro'))\n",
        "print(\"F1_Macro:\", f1_score(y_true, y_pred, average='macro'))\n",
        "print(\"Multi-label Accuracy (or Jaccard Index):\", jaccard_score(y_true,y_pred, average='samples'))\n",
        "print(\"Hamming_loss:\", hamming_loss(y_true, y_pred))    \n",
        "print(\"Precision_score\",precision_score(y_true, y_pred, average='samples'))   \n",
        "print(\"Recall_score\",recall_score(y_true, y_pred, average='samples'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLDCgK2gMgtf"
      },
      "source": [
        "F1_Micro: 0.4591018444266239\n",
        "F1_Macro: 0.3461348852045126\n",
        "Multi-label Accuracy (or Jaccard Index): 0.3595005528934758\n",
        "Hamming_loss: 0.03551682375862251"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZkt2jDgIgPC"
      },
      "source": [
        "F1_Micro: 0.45789420857171537\n",
        "F1_Macro: 0.34420029442631483\n",
        "Multi-label Accuracy (or Jaccard Index): 0.35919338985133303\n",
        "Hamming_loss: 0.035549734084566374"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQQGWzYvrt7k"
      },
      "source": [
        "#**SGDClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAjQ9bEuzjdE"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "  ## Define Classifier\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "\n",
        "\n",
        "\n",
        "classifier = Pipeline([\n",
        "     ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "\t ('tfidf', TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)),\n",
        "     ('clf', OneVsRestClassifier(SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=200)))])\n",
        "\n",
        "\n",
        "\n",
        "# print(X_train)\n",
        "\n",
        "\n",
        "df_predicted=pd.DataFrame()\n",
        "df_groundTruth=pd.DataFrame()\n",
        "\n",
        "for category in categories:\n",
        "  # print('... Processing Category: {}'.format(category))\n",
        "  # train the model using X_dtm & y\n",
        "  classifier.fit(X_train, tmp_train[category])\n",
        "  \n",
        "# compute the testing accuracy\n",
        "  prediction = classifier.predict(X_test)\n",
        "  # print(prediction)\n",
        "  df_predicted[category]=prediction\n",
        "  #print(df_predicted)\n",
        "  df_groundTruth[category]=tmp_test[category]\n",
        "  # print('F1 Micro: {}'.format(f1_score(test[category], prediction, average='micro')))\n",
        "\n",
        "\n",
        "# print(df_predicted)\n",
        "# print(df_groundTruth)\n",
        "\n",
        "y_true = np.array(df_groundTruth)\n",
        "y_pred = np.array(df_predicted)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(y_true)\n",
        "# print(y_pred)\n",
        "\n",
        "# print(\"\\n\")\n",
        "print(\"F1_Micro:\", f1_score(y_true, y_pred, average='micro'))\n",
        "print(\"F1_Macro:\", f1_score(y_true, y_pred, average='macro'))\n",
        "print(\"Multi-label Accuracy (or Jaccard Index):\", jaccard_score(y_true,y_pred, average='samples'))\n",
        "print(\"Hamming_loss:\", hamming_loss(y_true, y_pred))    \n",
        "print(\"Precision_score\",precision_score(y_true, y_pred, average='samples'))   \n",
        "print(\"Recall_score\",recall_score(y_true, y_pred, average='samples'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC8xO_I0KXvK"
      },
      "source": [
        "F1_Micro: 0.28007838014369696\n",
        "F1_Macro: 0.18643566773984335\n",
        "Multi-label Accuracy (or Jaccard Index): 0.16826391448580907\n",
        "Hamming_loss: 0.03627376125533147"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqrLJ7WRr1MB"
      },
      "source": [
        "#**DecisionTreeClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZL24k3LzkoR"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "  ## Define Classifier\n",
        "#Ref.: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifier = Pipeline([\n",
        "     ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "\t ('tfidf', TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)),\n",
        "     ('clf', OneVsRestClassifier(DecisionTreeClassifier(random_state=0)))])\n",
        "\n",
        "\n",
        "\n",
        "# print(X_train)\n",
        "\n",
        "\n",
        "df_predicted=pd.DataFrame()\n",
        "df_groundTruth=pd.DataFrame()\n",
        "\n",
        "for category in categories:\n",
        "  print('... Processing Category: {}'.format(category))\n",
        "  # train the model using X_dtm & y\n",
        "  classifier.fit(X_train, tmp_train[category])\n",
        "  \n",
        "# compute the testing accuracy\n",
        "  prediction = classifier.predict(X_test)\n",
        "  #print(prediction)\n",
        "  df_predicted[category]=prediction\n",
        "  df_groundTruth[category]=tmp_test[category]\n",
        "  print('F1 Macro: ',(f1_score(tmp_test[category], prediction, average='macro')))\n",
        "\n",
        "\n",
        "# print(df_predicted)\n",
        "# print(df_groundTruth)\n",
        "\n",
        "y_true = np.array(df_groundTruth)\n",
        "y_pred = np.array(df_predicted)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(y_true)\n",
        "# print(y_pred)\n",
        "\n",
        "# print(\"\\n\")\n",
        "print(\"F1_Micro:\", f1_score(y_true, y_pred, average='micro'))\n",
        "print(\"F1_Macro:\", f1_score(y_true, y_pred, average='macro'))\n",
        "print(\"Multi-label Accuracy (or Jaccard Index):\", jaccard_score(y_true,y_pred, average='samples'))\n",
        "print(\"Hamming_loss:\", hamming_loss(y_true, y_pred))    \n",
        "print(\"Precision_score\",precision_score(y_true, y_pred, average='samples'))   \n",
        "print(\"Recall_score\",recall_score(y_true, y_pred, average='samples'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IspcB0VUZaAy"
      },
      "source": [
        "F1_Micro: 0.43684295649361155\n",
        "F1_Macro: 0.36163879876542265\n",
        "Multi-label Accuracy (or Jaccard Index): 0.35750399311954784\n",
        "Hamming_loss: 0.045837501974619554"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M7aAtr7khle1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define Classifier\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "classifier = Pipeline([\n",
        "    ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "    ('tfidf', TfidfTransformer(norm='l2', use_idf=True,\n",
        "     smooth_idf=True, sublinear_tf=False)),\n",
        "    ('clf', OneVsRestClassifier(MLPClassifier(hidden_layer_sizes=(10,), random_state=1, max_iter=60, warm_start=True)))])\n",
        "\n",
        "\n",
        "# print(X_train)\n",
        "\n",
        "\n",
        "df_predicted = pd.DataFrame()\n",
        "df_groundTruth = pd.DataFrame()\n",
        "\n",
        "for category in categories:\n",
        "    print('... Processing Category: {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    classifier.fit(X_train, tmp_train[category])\n",
        "\n",
        "# compute the testing accuracy\n",
        "    prediction = classifier.predict(X_test)\n",
        "    # print(prediction)\n",
        "    df_predicted[category] = prediction\n",
        "    df_groundTruth[category] = tmp_test[category]\n",
        "    # print('F1 Micro: {}'.format(f1_score(test[category], prediction, average='micro')))\n",
        "\n",
        "\n",
        "# print(df_predicted)\n",
        "# print(df_groundTruth)\n",
        "y_true = np.array(df_groundTruth)\n",
        "y_pred = np.array(df_predicted)\n",
        "\n",
        "\n",
        "# print(y_true)\n",
        "# print(y_pred)\n",
        "\n",
        "# print(\"\\n\")\n",
        "print(\"F1_Micro:\", f1_score(y_true, y_pred, average='micro'))\n",
        "print(\"F1_Macro:\", f1_score(y_true, y_pred, average='macro'))\n",
        "print(\"Multi-label Accuracy (or Jaccard Index):\",\n",
        "      jaccard_score(y_true, y_pred, average='samples'))\n",
        "print(\"Hamming_loss:\", hamming_loss(y_true, y_pred))"
      ],
      "metadata": {
        "id": "bA0Nfu_DhiJu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}